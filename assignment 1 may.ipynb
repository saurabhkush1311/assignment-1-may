{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a26a52-0684-43f3-86b5-61d175986ee9",
   "metadata": {},
   "source": [
    "# ANSWER 1\n",
    "A contingency matrix, also known as a confusion matrix, is a table that is often used to evaluate the performance of a classification model. It shows the number of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) predictions made by the model for each class in a classification problem.\n",
    "\n",
    "By comparing the predicted class labels to the actual class labels, the contingency matrix helps to measure various evaluation metrics such as accuracy, precision, recall, F1 score, etc. It provides a comprehensive view of the model's performance, especially when dealing with imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d498bf4-d5cf-4264-8337-fbf4076d87f3",
   "metadata": {},
   "source": [
    "# ANSWER 2\n",
    "A pair confusion matrix is used in multi-label classification tasks, where each instance can belong to more than one class simultaneously (e.g., document categorization with multiple tags). It extends the concept of a regular confusion matrix to handle multiple true and predicted labels for each instance.\n",
    "\n",
    "In a pair confusion matrix, each entry represents the number of instances that have specific pairs of true and predicted labels. This enables the evaluation of metrics that consider label co-occurrences, such as precision at k, recall at k, etc. Pair confusion matrices are useful when dealing with multi-label classification tasks and assessing the performance of models that predict multiple labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea660286-50ce-40a0-bb18-0b3b030faaae",
   "metadata": {},
   "source": [
    "# ANSWER 3\n",
    "In NLP, an extrinsic measure evaluates a language model's performance in the context of a downstream task or application. Instead of measuring the model's performance on a specific NLP task directly, an extrinsic measure evaluates how well the model's outputs contribute to the performance of the downstream task.\n",
    "\n",
    "For example, in machine translation, an extrinsic measure would evaluate how well the translation model performs in improving the overall translation quality rather than measuring its performance on translation accuracy alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5b832-c7cb-4f0e-9f19-a55d4076020a",
   "metadata": {},
   "source": [
    "# ANSWER 4\n",
    "Intrinsic measures in machine learning evaluate the performance of a model based on its performance on a specific task without considering its impact on any downstream applications. These measures are task-specific and typically focus on aspects such as accuracy, precision, recall, F1 score, etc., for classification tasks, or mean squared error, R-squared, etc., for regression tasks.\n",
    "\n",
    "Extrinsic measures, on the other hand, assess a model's performance based on how well it aids or improves the performance of a higher-level task or application. They evaluate the model's contribution to solving a more significant problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98388d-3273-4311-95ab-04d489390e8a",
   "metadata": {},
   "source": [
    "# ANSWER 5\n",
    "A confusion matrix is used to evaluate the performance of a classification model by summarizing the model's predictions against the true class labels.\n",
    "\n",
    "It helps in calculating various performance metrics such as:\n",
    "1. Accuracy: The proportion of correctly classified instances out of the total instances.\n",
    "2. Precision: The proportion of true positive predictions out of all positive predictions.\n",
    "3. Recall (Sensitivity): The proportion of true positive predictions out of all actual positive instances.\n",
    "4. F1 score: The harmonic mean of precision and recall, balancing both metrics.\n",
    "\n",
    "The confusion matrix provides insights into the model's performance, helping to identify areas of strength and weakness, such as which classes the model struggles to predict accurately and whether the model is biased towards any specific class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869aee05-231d-4152-834d-4d8c7603d4db",
   "metadata": {},
   "source": [
    "# ANSWER 6\n",
    "For unsupervised learning algorithms, intrinsic measures evaluate the quality of clustering or dimensionality reduction directly without relying on external labels. \n",
    "\n",
    "Some common intrinsic measures include:\n",
    "1. Silhouette score: Measures how well-separated clusters are and ranges from -1 to 1, with higher scores indicating better-defined clusters.\n",
    "2. Davies-Bouldin index: Measures the average similarity between each cluster and its most similar cluster, with lower values indicating better clustering.\n",
    "3. Calinski-Harabasz index: Measures the ratio of between-cluster variance to within-cluster variance, with higher values indicating better-defined clusters.\n",
    "\n",
    "These measures help assess the quality of unsupervised learning results and provide insights into the appropriateness of the algorithm for the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f636285-96ec-4e60-937e-beff71b4b3da",
   "metadata": {},
   "source": [
    "# ANSWER 7\n",
    "Limitations of using accuracy as a sole evaluation metric for classification tasks:\n",
    "\n",
    "Imbalanced datasets: Accuracy can be misleading when dealing with imbalanced datasets, where one class dominates the others. A high overall accuracy may be achieved by just predicting the majority class, while the model's performance on the minority classes might be poor.\n",
    "\n",
    "Inadequate for cost-sensitive applications: Different misclassifications may have different consequences in real-world applications. Accuracy does not take into account the varying costs associated with false positives and false negatives.\n",
    "\n",
    "Focus on correct classification: Accuracy does not consider the confidence or the level of certainty in the model's predictions.\n",
    "\n",
    "To address these limitations, it is essential to use additional evaluation metrics such as precision, recall, F1 score, ROC-AUC, etc., based on the specific requirements of the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc100a-b151-49ee-82a5-8a14b786f106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06627a65-241a-41e6-b3ed-104f58711e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
